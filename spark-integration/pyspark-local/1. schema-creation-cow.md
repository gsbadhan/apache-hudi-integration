# COW table type

#### Note: below scripts based on pyspark console


```
from pyspark.sql.types import *
from pyspark.sql.functions import *
from datetime import datetime
```

```
# ===== 1. DEFINE SCHEMA ONCE =====
EMPLOYEE_SCHEMA = StructType([
    StructField("employee_id", IntegerType(), False),
    StructField("name", StringType(), False),
    StructField("department_id", IntegerType(), True),
    StructField("salary", DoubleType(), True),
    StructField("sales_amount", DoubleType(), True),
    StructField("performance_score", DoubleType(), True),
    StructField("last_updated", TimestampType(), True)
])
```

```
# ===== 2. DEFINE HUDI OPTIONS ONCE =====
HUDI_OPTIONS = {
    "hoodie.table.name": "employees",
    "hoodie.datasource.write.table.type": "COPY_ON_WRITE",
    "hoodie.datasource.write.recordkey.field": "employee_id",
    "hoodie.datasource.write.partitionpath.field": "department_id",
    "hoodie.datasource.write.precombine.field": "last_updated",
    "hoodie.datasource.write.operation": "upsert"
}
```
 
 ```
# ===== 3. DEFINE BASE PATH ONCE =====
BASE_PATH = "/tmp/hudi_employees_cow"
```

```
# Create initial employee data
initial_data = [
    (1, "John Doe", 101, 75000.0, 150000.0, 8.5, datetime(2024, 1, 1, 10, 0, 0)),
    (2, "Jane Smith", 102, 65000.0, 120000.0, 7.8, datetime(2024, 1, 1, 10, 0, 0)),
    (3, "Bob Johnson", 101, 80000.0, 180000.0, 9.2, datetime(2024, 1, 1, 10, 0, 0))
]
```

```
# Create DataFrame with predefined schema
initial_df = spark.createDataFrame(initial_data, EMPLOYEE_SCHEMA)
```

```
# Show initial data
print("=== INITIAL EMPLOYEE DATA ===")
initial_df.show()
```

```
# save data
initial_df.write.format("hudi").options(**HUDI_OPTIONS).mode("overwrite").save(BASE_PATH)
```

```
# read from store
empdf=spark.read.format("hudi").load(BASE_PATH)
empdf.select("employee_id", "name", "department_id", "salary").show()

empdf.show()
```

```
# read with filter query
df = spark.read.format("hudi").load(BASE_PATH).filter("department_id = 101")
```

```
# Add new employee - Need schema only for new DataFrame
new_employee = spark.createDataFrame(
    [(4, "Alice Brown", 103, 70000.0, 90000.0, 6.5, datetime.now())],
    EMPLOYEE_SCHEMA
)

new_employee.write.format("hudi").options(**HUDI_OPTIONS).mode("append").save(BASE_PATH)
```


```
# update existing record

update_data = [
    (1, "John Dow", 101, 85000.0, 160000.0, 9.0, datetime(2024, 1, 2, 14, 0, 0))
]

update_df = spark.createDataFrame(update_data, EMPLOYEE_SCHEMA)

update_df.write.format("hudi").options(**HUDI_OPTIONS).mode("append").save(BASE_PATH)

updated_df = spark.read.format("hudi").load(BASE_PATH)

updated_df.show()

```

```
# real time update
update_df=spark.read.format("hudi").load(BASE_PATH).filter("performance_score >= 7.0").withColumn("salary", col("salary")+1000).withColumn("last_updated", current_timestamp()).select(*EMPLOYEE_SCHEMA.fieldNames())

update_df.write.format("hudi").options(**HUDI_OPTIONS).mode("append").save(BASE_PATH)

updated_df = spark.read.format("hudi").load(BASE_PATH)

updated_df.show()

```

```
# delete record
deldf = spark.read.format("hudi").load(BASE_PATH).filter("employee_id=3")
options = HUDI_OPTIONS.copy()
options["hoodie.datasource.write.operation"] = "delete"
deldf.write.format("hudi").options(**options).mode("append").save(BASE_PATH)

deldf = spark.read.format("hudi").load(BASE_PATH)

deldf.show()

```

